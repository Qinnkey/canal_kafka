#spring.data.elasticsearch.cluster-name=java-es
#spring.data.elasticsearch.cluster-nodes=192.168.0.241:9300,192.168.0.251:9300,192.168.0.254:9300

#spring.data.elasticsearch.cluster-name=test
#spring.data.elasticsearch.cluster-nodes=192.168.56.101:9300
#
##elasticsearch日志存储目录
#spring.data.elasticsearch.properties.path.logs=./elasticsearch/log 
##elasticsearch数据存储目录
#spring.data.elasticsearch.properties.path.data=./elasticsearch/data
#spring.data.elasticsearch.repositories.enabled=true 


server.port=8003

#连接mysql
spring.datasource.url=jdbc:mysql://localhost:3306/dream_shop_new?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
mybatis.mapper-locations=classpath:/mapper/*.xml


spring.elasticsearch.cluster-name=es-java
spring.elasticsearch..cluster-nodes=192.168.0.241:9300,192.168.0.251:9300,192.168.0.254:9300
#

#kafka相关配置,ip地址
spring.kafka.bootstrap-servers=192.168.0.251:9092,192.168.0.242:9092
#设置一个默认组
spring.kafka.consumer.group-id=test-consumer-group


##key-value序列化反序列化
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
##每次fetch请求时，server应该返回的最小字节数
#spring.kafka.consumer.fetch-min-size=100
#spring.kafka.consumer.max-poll-records=1000
##关闭自动提交
#spring.kafka.consumer.auto-commit-interval=1000
#spring.kafka.consumer.enable-auto-commit=false
#配置 latest是消费新生产的数据， earliest是每个分区从头开始消费，none 当该topic下所有分区中存在未提交的offset时，抛出异常。
#spring.kafka.consumer.auto-offset-reset=earliest

#用于序列化
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#用来缓存生产者发送到服务器的自己数
spring.kafka.producer.buffer-memory=92428800
#生产者将在请求传输之间到达的任何记录组合成单个批量请求
spring.kafka.properties.linger.ms=50

#设置重新发送的时候，保证数据不丢失
spring.kafka.producer.retries=30
#用于压缩的数据类型
spring.kafka.producer.compression-type=gzip

#确认需要服务器的反馈，才能表示写入成功，这是保证不会丢失数据的最佳配置，如果是1的话，是等leader写入本地数据，其他的备份无法保证
spring.kafka.producer.acks=1


#用来增加缓存等信息
spring.kafka.properties.receive.buffer.bytes =88008888
spring.kafka.properties.send.buffer.bytes=88008888
spring.kafka.properties.max.request.size=40485760

#用于kafka消费端的索引和类型
elasticsearch.index=product
elasticsearch.type=jd


#用于数据库和表的过滤
canal.tableName=ds_product
canal.databaseName=dream_shop_new

#设置生产端的线程
kafka.producer.thread= 8

kafka.insert.topic = product_insert
kafka.update.topic = product_update
kafka.delete.topic = product_delete
canalConfig.zkServers=192.168.0.251:2181,192.168.0.242:2181
canalConfig.destination=example